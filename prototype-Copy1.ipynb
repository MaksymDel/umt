{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO0: use LSTM (2 layers) similarly to Artexte for direct comparison \n",
    "\n",
    "TODO1: separate all this stuff to different files and stick to the default trainer and config\n",
    "\n",
    "TODO2: use jsonnet config to pass dataset reader to both model and train command\n",
    "\n",
    "TODO3: during backtranslation, first greedly backtranslate with torch.not_grad() \n",
    "\n",
    "TODO4: then convert output into list of instances into \"Batch\" into tensor_dict\n",
    "\n",
    "TODO5: call model forward on this tensor_dict and get gradients\n",
    "\n",
    "TODO6: then repeat for other languages (for loop), but reuse the same variables to save GPU \n",
    "memory\n",
    "\n",
    "TODO7: finally refactor to get ready for transformers\n",
    "\n",
    "TODO8: use fairseq transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsupervised_translation.data.dataset_readers.unsupervised_translation_reader import UnsupervisedTranslationDatasetsReader\n",
    "from unsupervised_translation.models.unsupervised_translation import UnsupevisedTranslation\n",
    "from unsupervised_translation.data.iterators.homogeneous_batch_iterator import HomogeneousBatchIterator\n",
    "\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.modules.token_embedders.embedding import Embedding\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper\n",
    "import torch\n",
    "import allennlp\n",
    "from allennlp.training import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:00, 2709.64it/s]\n"
     ]
    }
   ],
   "source": [
    "umt_dr = UnsupervisedTranslationDatasetsReader(lazy=True)\n",
    "\n",
    "data_files = {\"en\": \"fixtures/data/mono.en\", \"ru\": \"fixtures/data/mono.ru\"}\n",
    "data_files = {\"en-et\": \"abracadabra\", \"en\": \"fixtures/data/mono.en\", \"ru\": \"fixtures/data/mono.ru\"}\n",
    "\n",
    "train_dataset = umt_dr.read(data_files)\n",
    "validation_dataset = umt_dr.read(data_files)\n",
    "\n",
    "vocab = Vocabulary.from_instances(train_dataset)\n",
    "\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "text_field_embedding = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "model = UnsupevisedTranslation(vocab=vocab, source_embedder=text_field_embedding, encoder=lstm, max_decoding_steps=10, target_namespace=\"tokens\")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "iterator = HomogeneousBatchIterator(type_field_name = \"lang\", batch_size = 2)\n",
    "iterator.index_with(vocab)\n",
    "iterator.get_num_batches(train_dataset)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 4.2367 ||: : 7it [00:00, 63.02it/s]\n",
      "BLEU: 0.0000, loss: 0.0000 ||: : 7it [00:00, 88.42it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ochumla', 'my', 'my', 'my', 'my', 'my', 'my', 'There', 'my'], ['ochumla', 'my', 'my', 'my', 'my', 'There', 'There', 'There', 'There']]\n",
      "[['my', 'my', 'my', 'my', 'my'], ['ochumla', 'my', 'my', 'my', 'my']]\n",
      "[['my', 'my', 'my', 'my', 'my', 'my'], ['my', 'my', 'my', 'my', 'my', 'my']]\n",
      "[['my', 'my', 'my', 'my', 'my', 'my'], ['my', 'my', 'my', 'my', 'my', 'my']]\n",
      "[['my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'There', 'There'], ['my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my']]\n",
      "[['my', 'my', 'my', 'my']]\n",
      "[['ochumla', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my']]\n",
      "[['ochumla', 'my', 'my', 'my', 'my', 'my', 'my', 'There', 'my'], ['ochumla', 'my', 'my', 'my', 'my', 'There', 'There', 'There', 'There']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 4.2076 ||: : 7it [00:00, 54.71it/s]\n",
      "BLEU: 0.0000, loss: 0.0000 ||: : 7it [00:00, 82.85it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'my', 'my', 'my', 'my'], ['ochumla', 'my', 'my', 'my', 'my']]\n",
      "[['my', 'my', 'my', 'my', 'my', 'my'], ['my', 'my', 'my', 'my', 'my', 'my']]\n",
      "[['my', 'my', 'my', 'my', 'my', 'my'], ['my', 'my', 'my', 'my', 'my', 'my']]\n",
      "[[',', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'There', 'There'], [',', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my']]\n",
      "[[',', 'my', 'my', 'my']]\n",
      "[['ochumla', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my']]\n",
      "[[',', ',', 'my', 'my', 'my', 'my', 'my', 'my', 'my'], [',', 'my', 'my', 'my', 'my', 'There', 'There', 'There', 'There']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 4.1791 ||: : 7it [00:00, 67.15it/s]\n",
      "BLEU: 0.0000, loss: 0.0000 ||: : 7it [00:00, 91.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[',', 'my', 'my', 'my', 'my'], [',', 'my', 'my', 'my', 'my']]\n",
      "[[',', ',', 'my', 'my', 'my', 'my'], [',', ',', 'my', 'my', 'my', 'my']]\n",
      "[[',', 'my', 'my', 'my', 'my', 'my'], [',', 'my', 'my', 'my', 'my', 'my']]\n",
      "[[',', ',', 'my', 'my', 'my', 'my', 'my', 'my', 'There', 'There'], [',', ',', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my']]\n",
      "[[',', 'my', 'my', 'my']]\n",
      "[[',', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 0,\n",
       " 'peak_cpu_memory_MB': 252.516,\n",
       " 'training_duration': '00:00:00',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 2,\n",
       " 'epoch': 2,\n",
       " 'training_loss': 4.179075990404401,\n",
       " 'training_cpu_memory_MB': 252.516,\n",
       " 'validation_BLEU': 1.7587783859894286e-12,\n",
       " 'validation_loss': 0.0,\n",
       " 'best_validation_BLEU': 1.7587783859894286e-12,\n",
       " 'best_validation_loss': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common.util import JsonDict\n",
    "from allennlp.data import Instance\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "\n",
    "class MySeq2SeqPredictor(Predictor):\n",
    "    \"\"\"\n",
    "    Predictor for sequence to sequence models, including\n",
    "    :class:`~allennlp.models.encoder_decoder.simple_seq2seq` and\n",
    "    :class:`~allennlp.models.encoder_decoder.copynet_seq2seq`.\n",
    "    \"\"\"\n",
    "\n",
    "    def predict(self, source: str, lang: str) -> JsonDict:\n",
    "        return self.predict_json({\"lang\": lang, \"source\" : source})\n",
    "\n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        \"\"\"\n",
    "        Expects JSON that looks like ``{\"lang\", \"source\": \"...\"}``.\n",
    "        \"\"\"\n",
    "        source = json_dict[\"source\"]\n",
    "        lang = json_dict[\"lang\"]\n",
    "        return self._dataset_reader.text_to_instance(source, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MySeq2SeqPredictor(model, UnsupervisedTranslationDatasetsReader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_log_probabilities': [-37.89716339111328],\n",
       " 'predictions': [[40, 40, 40, 40, 40, 40, 40, 40, 40, 40]],\n",
       " 'predicted_tokens': ['my',\n",
       "  'my',\n",
       "  'my',\n",
       "  'my',\n",
       "  'my',\n",
       "  'my',\n",
       "  'my',\n",
       "  'my',\n",
       "  'my',\n",
       "  'my']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(\"There are two mouses in front of a cat.\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:inter]",
   "language": "python",
   "name": "conda-env-inter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
